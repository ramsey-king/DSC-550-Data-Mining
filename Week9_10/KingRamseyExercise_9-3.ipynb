{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ramsey King\n",
    "# DSC 550 - Data Mining\n",
    "# November 7, 2021\n",
    "# Exercise 9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>Barely better than Gabbert? He was significant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt\n",
       "0  sports  Barely better than Gabbert? He was significant..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = pd.read_json('categorized-comments.jsonl', lines=True)\n",
    "data.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['science_and_technology', 'sports', 'video_games']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_list = list(set(data['cat']))\n",
    "category_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Neural Network Classifier with Scikit\n",
    "\n",
    "Using the multi-label classifier dataset (categorized-comments.jsonl), fit a neural network classifier using scikit-learn to predict the comment category. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guide, but you will need to modify the code for this dataset. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Barely', 'RB'), ('better', 'RBR'), ('Gabbert', 'NNP'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def documents(corpus_text):\n",
    "    # retrieves the pickled, part-of-speech tagged documents from our corpus reader object\n",
    "    # Need to make a column that is part-of-speech tagged\n",
    "    # convert to lower case\n",
    "    normalized_text = corpus_text.lower()\n",
    "\n",
    "    # remove numbers\n",
    "    normalized_text = re.sub(r'\\d+', '', normalized_text)\n",
    "\n",
    "    # remove punctuation\n",
    "    normalized_text = re.sub(r'[^\\w\\s]','', normalized_text)\n",
    "\n",
    "    # remove whitespace\n",
    "    normalized_text = normalized_text.strip()\n",
    "\n",
    "    normalized_text = sent_tokenize(corpus_text)\n",
    "    for i in normalized_text:\n",
    "      \n",
    "        # Word tokenizers is used to find the words \n",
    "        # and punctuation in a string\n",
    "        wordsList = nltk.word_tokenize(i)\n",
    "    \n",
    "        # removing stop words from wordList\n",
    "        wordsList = [w for w in wordsList if not w in stop_words] \n",
    "        # wordsList = [w for w in wordsList] \n",
    "    \n",
    "        #  Using a Tagger. Which is part-of-speech \n",
    "        # tagger or POS-tagger. \n",
    "        tagged = nltk.pos_tag(wordsList)\n",
    "  \n",
    "        return tagged\n",
    "\n",
    "def normalize_text(corpus_text):\n",
    "    # retrieves the pickled, part-of-speech tagged documents from our corpus reader object\n",
    "    # Need to make a column that is part-of-speech tagged\n",
    "    # convert to lower case\n",
    "    normalized_text = corpus_text.lower()\n",
    "\n",
    "    # remove numbers\n",
    "    normalized_text = re.sub(r'\\d+', '', normalized_text)\n",
    "\n",
    "    # remove punctuation\n",
    "    normalized_text = re.sub(r'[^\\w\\s]','', normalized_text)\n",
    "\n",
    "    # remove whitespace\n",
    "    normalized_text = normalized_text.strip()\n",
    "\n",
    "    normalized_text = sent_tokenize(corpus_text)\n",
    "    for i in normalized_text:\n",
    "      \n",
    "        # Word tokenizers is used to find the words \n",
    "        # and punctuation in a string\n",
    "        wordsList = nltk.word_tokenize(i)\n",
    "    \n",
    "        # removing stop words from wordList\n",
    "        wordsList = [w for w in wordsList if not w in stop_words] \n",
    "        \n",
    "        return wordsList\n",
    "\n",
    "'''def continuous(corpus):\n",
    "    # to get the numeric ratings of each album\n",
    "    return list(corpus.scores())'''\n",
    "\n",
    "def make_categorical(corpus_cat):\n",
    "    cat_dictionary = {\n",
    "        'science_and_technology': 1,\n",
    "        'sports': 2,\n",
    "        'video_games': 3,\n",
    "    }\n",
    "\n",
    "    return cat_dictionary.get(corpus_cat)\n",
    "    \n",
    "data['cat_num'] = data['cat'].apply(make_categorical)\n",
    "data['tokenized_txt'] = data['txt'].apply(lambda x: documents(x))\n",
    "data['normalized_txt'] = data['txt'].apply(lambda x: normalize_text(x))\n",
    "print(data['tokenized_txt'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get the scores required to satsify the exercise requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def train_model(path, model, continuous=True, saveto=None, cv=12):\n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path; constructing cross-validation\n",
    "    scores using the cv parameter, then fitting the model on the full data.\n",
    "    Returns the scores.\n",
    "    \"\"\"\n",
    "    # Load the corpus data and labels for classification\n",
    "    # corpus = PickledReviewsReader(path)\n",
    "    corpus = data['tokenized_txt']\n",
    "    # X = documents(corpus)\n",
    "    X = data['tokenized_txt']\n",
    "    '''\n",
    "    if continuous:\n",
    "        y = continuous(corpus)\n",
    "        scoring = 'r2_score'\n",
    "    else:\n",
    "        y = make_categorical(corpus)\n",
    "        scoring = 'f1_score'\n",
    "    '''\n",
    "    y = data['cat_num']\n",
    "    # Compute cross-validation scores\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "\n",
    "    # Write to disk if specified\n",
    "    if saveto:\n",
    "        joblib.dump(model, saveto)\n",
    "\n",
    "    # Fit the model on entire dataset\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Return scores\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0ea00e83ed07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'norm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'normalized_txt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'tfidf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[1;34m'ann'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMLPRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m ])\n\u001b[0;32m     16\u001b[0m \u001b[0mregression_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontinuous\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, steps, memory, verbose)\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_validate_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             if not (hasattr(t, \"fit\") or hasattr(t, \"fit_transform\")) or not hasattr(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1442\u001b[0m         raise ValueError(\n\u001b[1;32m-> 1443\u001b[1;33m             \u001b[1;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1444\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "\n",
    "# from transformer import TextNormalizer\n",
    "# from reader import PickledReviewsReader\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Path to postpreprocessed, part-of-speech tagged review corpus\n",
    "cpath = data['tokenized_txt']\n",
    "\n",
    "regressor = Pipeline([\n",
    "    ('norm', data['normalized_txt']),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('ann', MLPRegressor(hidden_layer_sizes=[500,150], verbose=True))\n",
    "])\n",
    "regression_scores = train_model(cpath, regressor, continuous=True)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('norm', data['normalized_txt']),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('ann', MLPClassifier(hidden_layer_sizes=[500,150], verbose=True))\n",
    "])\n",
    "classifer_scores = train_model(cpath, classifier, continuous=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf60427b139290154779a7e38fd0f127f78e5a9dfc17b4a4bd1d849158c5fa70"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
