{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ramsey King\n",
    "# DSC 550 - Data Mining\n",
    "# November 7, 2021\n",
    "# Exercise 9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>Barely better than Gabbert? He was significant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cat                                                txt\n",
       "0  sports  Barely better than Gabbert? He was significant..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data = pd.read_json('categorized-comments.jsonl', lines=True)\n",
    "data.head(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['science_and_technology', 'sports', 'video_games']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_list = list(set(data['cat']))\n",
    "category_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Neural Network Classifier with Scikit\n",
    "\n",
    "Using the multi-label classifier dataset (categorized-comments.jsonl), fit a neural network classifier using scikit-learn to predict the comment category. Use the code found in chapter 12 of the Applied Text Analysis with Python book as a guide, but you will need to modify the code for this dataset. Report the accuracy, precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         [(Barely, RB), (better, JJR), (than, IN), (Gab...\n",
      "1         [(Fuck, IN), (the, DT), (ducks, NNS), (and, CC...\n",
      "2         [(Should, MD), (have, VB), (drafted, VBN), (mo...\n",
      "3         [([, NN), (Done, NNP), (], NNP), ((, (), (http...\n",
      "4                                [(No, DT), (!, .), (!, .)]\n",
      "                                ...                        \n",
      "606461                                       [(touche, NN)]\n",
      "606462    [(Not, RB), (me, PRP), (,, ,), (I, PRP), (coul...\n",
      "606463    [(you, PRP), (obviously, RB), (do, VBP), (n't,...\n",
      "606464    [(I, PRP), ('m, VBP), (not, RB), (an, DT), (an...\n",
      "606465    [(Does, VBZ), (it, PRP), (have, VB), (gnomes, ...\n",
      "Name: tokenized_txt, Length: 606466, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def documents(corpus_text):\n",
    "    # retrieves the pickled, part-of-speech tagged documents from our corpus reader object\n",
    "    # Need to make a column that is part-of-speech tagged\n",
    "    tokenized = sent_tokenize(corpus_text)\n",
    "    for i in tokenized:\n",
    "      \n",
    "        # Word tokenizers is used to find the words \n",
    "        # and punctuation in a string\n",
    "        wordsList = nltk.word_tokenize(i)\n",
    "    \n",
    "        # removing stop words from wordList\n",
    "        # wordsList = [w for w in wordsList if not w in stop_words] \n",
    "        # wordsList = [w for w in wordsList] \n",
    "    \n",
    "        #  Using a Tagger. Which is part-of-speech \n",
    "        # tagger or POS-tagger. \n",
    "        tagged = nltk.pos_tag(wordsList)\n",
    "  \n",
    "        return tagged\n",
    "\n",
    "def continuous(corpus):\n",
    "    # to get the numeric ratings of each album\n",
    "    return list(corpus.scores())\n",
    "\n",
    "def make_categorical(corpus_cat):\n",
    "    cat_dictionary = {\n",
    "        'science_and_technology': 1,\n",
    "        'sports': 2,\n",
    "        'video_games': 3,\n",
    "    }\n",
    "\n",
    "    return cat_dictionary.get(corpus_cat)\n",
    "    \n",
    "data['cat_num'] = data['cat'].apply(make_categorical)\n",
    "data['tokenized_txt'] = data['txt'].apply(lambda x: documents(x))\n",
    "print(data['tokenized_txt'][:-10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get the scores required to satsify the exercise requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def train_model(path, model, continuous=True, saveto=None, cv=12):\n",
    "    \"\"\"\n",
    "    Trains model from corpus at specified path; constructing cross-validation\n",
    "    scores using the cv parameter, then fitting the model on the full data.\n",
    "    Returns the scores.\n",
    "    \"\"\"\n",
    "    # Load the corpus data and labels for classification\n",
    "    corpus = PickledReviewsReader(path)\n",
    "    X = documents(corpus)\n",
    "    if continuous:\n",
    "        y = continuous(corpus)\n",
    "        scoring = 'r2_score'\n",
    "    else:\n",
    "        y = make_categorical(corpus)\n",
    "        scoring = 'f1_score'\n",
    "\n",
    "    # Compute cross-validation scores\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n",
    "\n",
    "    # Write to disk if specified\n",
    "    if saveto:\n",
    "        joblib.dump(model, saveto)\n",
    "\n",
    "    # Fit the model on entire dataset\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Return scores\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformer import TextNormalizer\n",
    "from reader import PickledReviewsReader\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Path to postpreprocessed, part-of-speech tagged review corpus\n",
    "cpath = data['tokenized_txt']\n",
    "\n",
    "regressor = Pipeline([\n",
    "    ('norm', TextNormalizer()),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('ann', MLPRegressor(hidden_layer_sizes=[500,150], verbose=True))\n",
    "])\n",
    "regression_scores = train_model(cpath, regressor, continuous=True)\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('norm', TextNormalizer()),\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('ann', MLPClassifier(hidden_layer_sizes=[500,150], verbose=True))\n",
    "])\n",
    "classifer_scores = train_model(cpath, classifier, continuous=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf60427b139290154779a7e38fd0f127f78e5a9dfc17b4a4bd1d849158c5fa70"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
